{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439f3df0",
   "metadata": {},
   "source": [
    "# Assignment: Predicting Loan Default Using PySpark MLlib\n",
    "End-to-end, minimal-shuffle, Colab/Dataproc/local friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d40f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on Colab, install PySpark\n",
    "import sys, os, subprocess, pathlib, numpy as np, pandas as pd\n",
    "if 'google.colab' in sys.modules:\n",
    "    try:\n",
    "        import pyspark  # noqa: F401\n",
    "    except Exception:\n",
    "        print(\"Installing PySpark...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pyspark==3.5.1\"], check=True)\n",
    "\n",
    "# Generate CSV if not present\n",
    "DATA_PATH = \"loan_data.csv\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    rng = np.random.default_rng(42)\n",
    "    N = 5000\n",
    "    def weighted_choice(options, probs):\n",
    "        return rng.choice(options, size=N, p=probs)\n",
    "\n",
    "    age = rng.integers(21, 69, size=N)\n",
    "    gender = weighted_choice([\"Male\", \"Female\"], [0.55, 0.45])\n",
    "    income = np.round(rng.normal(loc=60000, scale=20000, size=N).clip(15000, 200000), 2)\n",
    "    loan_amount = np.round((income * rng.uniform(0.1, 0.6, size=N)) + rng.normal(0, 3000, size=N), 2).clip(1000, 300000)\n",
    "    loan_term = rng.integers(12, 360, size=N)\n",
    "    credit_score = np.round(np.clip(rng.normal(680, 60, size=N), 300, 850), 1)\n",
    "    employment_status = weighted_choice([\"Employed\", \"Unemployed\", \"Self-employed\", \"Student\"], [0.65, 0.08, 0.22, 0.05])\n",
    "    marital_status = weighted_choice([\"Single\", \"Married\", \"Divorced\"], [0.45, 0.45, 0.10])\n",
    "    dti = loan_amount / (income + 1e-6)\n",
    "    base = 1.5 - (credit_score - 680) / 120 + 0.35 * (dti - 0.35) + 0.1 * ((age < 25) * 1) + 0.05 * ((age > 60) * 1)\n",
    "    emp_adj = np.select([employment_status == \"Unemployed\", employment_status == \"Student\", employment_status == \"Self-employed\"],\n",
    "                        [0.6, 0.3, 0.1], default=0.0)\n",
    "    mar_adj = np.where(marital_status == \"Married\", -0.05, 0.05)\n",
    "    logit = base + emp_adj + mar_adj + rng.normal(0, 0.25, size=N)\n",
    "    pd_default = 1 / (1 + np.exp(-logit))\n",
    "    default = np.where(rng.random(N) < pd_default, \"Yes\", \"No\")\n",
    "    customerID = [f\"C{100000 + i}\" for i in range(N)]\n",
    "    df = pd.DataFrame({\n",
    "        \"customerID\": customerID,\n",
    "        \"age\": age.astype(int),\n",
    "        \"gender\": gender,\n",
    "        \"income\": income.astype(float),\n",
    "        \"loan_amount\": loan_amount.astype(float),\n",
    "        \"loan_term\": loan_term.astype(int),\n",
    "        \"credit_score\": credit_score.astype(float),\n",
    "        \"employment_status\": employment_status,\n",
    "        \"marital_status\": marital_status,\n",
    "        \"default\": default\n",
    "    })\n",
    "    df.to_csv(DATA_PATH, index=False)\n",
    "    print(f\"Wrote {DATA_PATH} with shape\", df.shape)\n",
    "else:\n",
    "    print(\"Found existing\", DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf339ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "DATA_PATH = \"loan_data.csv\"\n",
    "SEED = 42\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"LoanDefault_PySpark_MLlib\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"64\")\n",
    "         .getOrCreate())\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "df = (spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(DATA_PATH))\n",
    "\n",
    "# clean / cast\n",
    "for c in df.columns:\n",
    "    df = df.withColumnRenamed(c, c.strip().lower().replace(\" \", \"_\"))\n",
    "string_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, T.StringType)]\n",
    "for c in string_cols:\n",
    "    df = df.withColumn(c, F.trim(F.col(c)))\n",
    "cast_map = {\"age\": T.IntegerType(), \"income\": T.DoubleType(), \"loan_amount\": T.DoubleType(),\n",
    "            \"loan_term\": T.IntegerType(), \"credit_score\": T.DoubleType()}\n",
    "for c, t in cast_map.items():\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(c, F.col(c).cast(t))\n",
    "\n",
    "required = [\"customerid\",\"age\",\"gender\",\"income\",\"loan_amount\",\"loan_term\",\"credit_score\",\"employment_status\",\"marital_status\",\"default\"]\n",
    "for c in required:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(\"Missing column: \" + c)\n",
    "\n",
    "df = df.dropDuplicates()\n",
    "for c in required:\n",
    "    df = df.filter(F.col(c).isNotNull())\n",
    "\n",
    "df = df.cache()\n",
    "print(\"Rows:\", df.count())\n",
    "\n",
    "cat_cols = [\"gender\",\"employment_status\",\"marital_status\"]\n",
    "idx_cols = [c + \"_idx\" for c in cat_cols]\n",
    "ohe_cols = [c + \"_ohe\" for c in cat_cols]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\") for c in cat_cols]\n",
    "encoder = OneHotEncoder(inputCols=idx_cols, outputCols=ohe_cols, handleInvalid=\"keep\")\n",
    "label_indexer = StringIndexer(inputCol=\"default\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "num_cols = [\"age\",\"income\",\"loan_amount\",\"loan_term\",\"credit_score\"]\n",
    "assembler = VectorAssembler(inputCols=num_cols + ohe_cols, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "\n",
    "train, test = df.randomSplit([0.7, 0.3], seed=SEED)\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=50)\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", seed=SEED, numTrees=200, maxDepth=8, subsamplingRate=0.8)\n",
    "\n",
    "pipeline_lr = Pipeline(stages=indexers + [encoder, label_indexer, assembler, lr])\n",
    "pipeline_rf = Pipeline(stages=indexers + [encoder, label_indexer, assembler, rf])\n",
    "\n",
    "model_lr = pipeline_lr.fit(train)\n",
    "model_rf = pipeline_rf.fit(train)\n",
    "\n",
    "pred_lr = model_lr.transform(test).cache()\n",
    "pred_rf = model_rf.transform(test).cache()\n",
    "\n",
    "bin_eval = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "acc_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "f1_eval  = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "def metrics(pred):\n",
    "    auc = bin_eval.evaluate(pred)\n",
    "    acc = acc_eval.evaluate(pred)\n",
    "    f1  = f1_eval.evaluate(pred)\n",
    "    cm = (pred.select(F.col(\"label\").cast(\"int\").alias(\"y\"), F.col(\"prediction\").cast(\"int\").alias(\"p\"))\n",
    "               .groupBy(\"y\",\"p\").count())\n",
    "    return auc, acc, f1, cm\n",
    "\n",
    "auc_lr, acc_lr, f1_lr, cm_lr = metrics(pred_lr)\n",
    "auc_rf, acc_rf, f1_rf, cm_rf = metrics(pred_rf)\n",
    "\n",
    "print(\"\\\\n=== Logistic Regression ===\")\n",
    "print(f\"AUC={auc_lr:.4f}  ACC={acc_lr:.4f}  F1={f1_lr:.4f}\")\n",
    "cm_lr.orderBy(\"y\",\"p\").show()\n",
    "\n",
    "print(\"\\\\n=== Random Forest ===\")\n",
    "print(f\"AUC={auc_rf:.4f}  ACC={acc_rf:.4f}  F1={f1_rf:.4f}\")\n",
    "cm_rf.orderBy(\"y\",\"p\").show()\n",
    "\n",
    "# CV on LR (tight grid)\n",
    "param_grid_lr = (ParamGridBuilder()\n",
    "                 .addGrid(lr.regParam, [0.0, 0.01, 0.1])\n",
    "                 .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "                 .build())\n",
    "cv_lr = CrossValidator(estimator=pipeline_lr, estimatorParamMaps=param_grid_lr,\n",
    "                       evaluator=bin_eval, numFolds=3, seed=SEED, parallelism=2)\n",
    "\n",
    "cv_lr_model = cv_lr.fit(train)\n",
    "cv_auc = bin_eval.evaluate(cv_lr_model.transform(test))\n",
    "print(f\"\\\\nCV LR best AUC={cv_auc:.4f}\")\n",
    "\n",
    "# Feature importances for RF\n",
    "rf_fitted = [s for s in model_rf.stages if isinstance(s, RandomForestClassificationModel)][0]\n",
    "importances = rf_fitted.featureImportances\n",
    "feature_cols = num_cols + ohe_cols\n",
    "print(\"\\\\nTop numeric feature importances (RF):\")\n",
    "for i, c in enumerate(num_cols):\n",
    "    print(f\"{c:15s}: {importances[i]:.6f}\")\n",
    "\n",
    "# Save & reload model\n",
    "SAVE_PATH = \"models/rf_default_pipeline\"\n",
    "import shutil, os\n",
    "if os.path.exists(SAVE_PATH):\n",
    "    shutil.rmtree(SAVE_PATH)\n",
    "model_rf.write().overwrite().save(SAVE_PATH)\n",
    "reloaded = PipelineModel.load(SAVE_PATH)\n",
    "print(\"Reload OK:\", reloaded.transform(test).select(\"prediction\").limit(1).count())\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
