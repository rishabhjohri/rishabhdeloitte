{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443ced5d",
   "metadata": {},
   "source": [
    "\n",
    "# Customer Churn Prediction — PySpark MLlib (End-to-End)\n",
    "\n",
    "**What this notebook does**  \n",
    "1) Load `/mnt/data/churn.csv`  \n",
    "2) Clean & prepare data (StringIndexer + OneHotEncoder + VectorAssembler)  \n",
    "3) Train/test split (70/30)  \n",
    "4) Train **Logistic Regression, Decision Tree, Random Forest**  \n",
    "5) Evaluate with **AUC, Accuracy, Precision, Recall** + **Confusion Matrix**  \n",
    "6) **Cross-Validation** on LR (light grid)  \n",
    "7) **Feature Importances** (DT/RF)  \n",
    "8) Save & load the best pipeline\n",
    "\n",
    "> Tested on PySpark 3.x. If Spark is not installed, install with `pip install pyspark`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f36a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Setup: SparkSession & Imports\n",
    "# -----------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Create SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Churn-MLlib-Notebook\")\n",
    "         # .config(\"spark.driver.memory\", \"4g\")  # uncomment if needed\n",
    "         .getOrCreate())\n",
    "\n",
    "data_path = \"/mnt/data/churn.csv\"  # set to your uploaded file path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d579b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Load & Quick EDA\n",
    "# -----------------------------\n",
    "df_raw = (spark.read\n",
    "          .option(\"header\", True)\n",
    "          .option(\"inferSchema\", True)\n",
    "          .csv(data_path))\n",
    "\n",
    "print(\"Schema:\")\n",
    "df_raw.printSchema()\n",
    "\n",
    "print(\"Sample rows:\")\n",
    "df_raw.show(10, truncate=False)\n",
    "\n",
    "# Clean TotalCharges (common issue: blank strings)\n",
    "df = df_raw.withColumn(\"total_charges\",\n",
    "                       F.when(F.length(F.trim(F.col(\"total_charges\"))) == 0, None)\n",
    "                        .otherwise(F.col(\"total_charges\")).cast(DoubleType())\n",
    "                      ) if \"total_charges\" in df_raw.columns else df_raw\n",
    "\n",
    "# Standardize whitespace in string columns\n",
    "for c, t in df.dtypes:\n",
    "    if t == \"string\":\n",
    "        df = df.withColumn(c, F.trim(F.col(c)))\n",
    "\n",
    "# Basic class balance if 'churn' exists\n",
    "if \"churn\" in [c.lower() for c in df.columns]:\n",
    "    churn_col = [c for c in df.columns if c.lower() == \"churn\"][0]\n",
    "    print(\"Churn counts:\")\n",
    "    df.groupBy(churn_col).count().orderBy(F.desc(\"count\")).show()\n",
    "else:\n",
    "    print(\"No 'churn' column detected (case-insensitive).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2479ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Feature Engineering\n",
    "# -----------------------------\n",
    "# Define label column\n",
    "label_col = [c for c in df.columns if c.lower() == \"churn\"]\n",
    "assert len(label_col) == 1, \"Label column 'churn' not found or ambiguous.\"\n",
    "label_col = label_col[0]\n",
    "\n",
    "# Identify numeric and categorical columns quickly\n",
    "# You can pin exact columns if you want stricter control.\n",
    "numeric_cols = [c for c, t in df.dtypes if t in (\"int\", \"bigint\", \"double\", \"float\")]\n",
    "numeric_cols = [c for c in numeric_cols if c != label_col]\n",
    "\n",
    "categorical_cols = [c for c, t in df.dtypes if t == \"string\"]\n",
    "categorical_cols = [c for c in categorical_cols if c != label_col]\n",
    "\n",
    "# Drop rows with nulls in critical columns (fast, robust)\n",
    "critical = categorical_cols + numeric_cols + [label_col]\n",
    "df = df.dropna(subset=critical)\n",
    "\n",
    "# Label indexer\n",
    "label_indexer = StringIndexer(inputCol=label_col, outputCol=\"label\", handleInvalid=\"keep\")\n",
    "\n",
    "# Index + OneHot for categoricals\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "encoder  = OneHotEncoder(inputCols=[f\"{c}_idx\" for c in categorical_cols],\n",
    "                         outputCols=[f\"{c}_oh\"  for c in categorical_cols])\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{c}_oh\" for c in categorical_cols] + numeric_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Train/test split\n",
    "train, test = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(\"Train:\", train.count(), \"Test:\", test.count())\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "print(\"Categorical columns:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd91052",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Models: LR, DT, RF\n",
    "# -----------------------------\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=50, regParam=0.0, elasticNetParam=0.0)\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\", maxDepth=8, minInstancesPerNode=10)\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=120, maxDepth=12, subsamplingRate=0.8)\n",
    "\n",
    "base_stages = [label_indexer] + indexers + [encoder, assembler]\n",
    "\n",
    "lr_pipe = Pipeline(stages=base_stages + [lr])\n",
    "dt_pipe = Pipeline(stages=base_stages + [dt])\n",
    "rf_pipe = Pipeline(stages=base_stages + [rf])\n",
    "\n",
    "lr_model = lr_pipe.fit(train)\n",
    "dt_model = dt_pipe.fit(train)\n",
    "rf_model = rf_pipe.fit(train)\n",
    "\n",
    "print(\"Models trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784bf222",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Evaluation\n",
    "# -----------------------------\n",
    "bce = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "macc = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "mpre = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"weightedPrecision\")\n",
    "mrec = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"weightedRecall\")\n",
    "\n",
    "def evaluate(model, name):\n",
    "    preds = model.transform(test).cache()\n",
    "    auc = bce.evaluate(preds)\n",
    "    acc = macc.evaluate(preds)\n",
    "    pre = mpre.evaluate(preds)\n",
    "    rec = mrec.evaluate(preds)\n",
    "    print(f\"\\n{name} — AUC: {auc:.4f} | Acc: {acc:.4f} | Prec: {pre:.4f} | Recall: {rec:.4f}\")\n",
    "    cm = (preds.groupBy(\"label\", \"prediction\").count().orderBy(\"label\",\"prediction\"))\n",
    "    print(\"Confusion Matrix [label, prediction, count]:\")\n",
    "    cm.show()\n",
    "    return {\"preds\": preds, \"auc\": auc, \"acc\": acc, \"pre\": pre, \"rec\": rec}\n",
    "\n",
    "lr_metrics = evaluate(lr_model, \"LogisticRegression\")\n",
    "dt_metrics = evaluate(dt_model, \"DecisionTree\")\n",
    "rf_metrics = evaluate(rf_model, \"RandomForest\")\n",
    "\n",
    "# Leaderboard by AUC\n",
    "from collections import OrderedDict\n",
    "leaderboard = OrderedDict(sorted({\n",
    "    \"LR\": lr_metrics[\"auc\"],\n",
    "    \"DT\": dt_metrics[\"auc\"],\n",
    "    \"RF\": rf_metrics[\"auc\"],\n",
    "}.items(), key=lambda x: x[1], reverse=True))\n",
    "print(\"\\nModel AUC Leaderboard:\", leaderboard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f0d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Cross-Validation (LR)\n",
    "# -----------------------------\n",
    "lr_cv = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=60)\n",
    "lr_cv_pipe = Pipeline(stages=base_stages + [lr_cv])\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr_cv.regParam, [0.0, 0.01, 0.1])\n",
    "             .addGrid(lr_cv.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(estimator=lr_cv_pipe,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=bce,\n",
    "                    numFolds=3,\n",
    "                    parallelism=2)\n",
    "\n",
    "cv_model = cv.fit(train)\n",
    "cv_metrics = evaluate(cv_model.bestModel, \"LR (CV best)\")\n",
    "\n",
    "print(\"Best LR Params:\")\n",
    "print(\"regParam:\", cv_model.bestModel.stages[-1]._java_obj.parent().getRegParam())\n",
    "print(\"elasticNetParam:\", cv_model.bestModel.stages[-1]._java_obj.parent().getElasticNetParam())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83139b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Feature Importances (DT/RF)\n",
    "# -----------------------------\n",
    "def print_top_importances(fitted_pipe, top_k=15):\n",
    "    last = fitted_pipe.stages[-1]\n",
    "    if hasattr(last, \"featureImportances\"):\n",
    "        importances = last.featureImportances\n",
    "        input_cols = assembler.getInputCols()\n",
    "        pairs = list(enumerate(importances.toArray()))\n",
    "        ranked = sorted(pairs, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        print(\"\\nTop feature importances:\")\n",
    "        for idx, val in ranked:\n",
    "            print(f\"{input_cols[idx]:<30} {val:.6f}\")\n",
    "\n",
    "print_top_importances(dt_model)\n",
    "print_top_importances(rf_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2314ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Save & Load Best Model\n",
    "# -----------------------------\n",
    "cand_models = [\n",
    "    (\"LR\", lr_model, lr_metrics[\"auc\"]),\n",
    "    (\"DT\", dt_model, dt_metrics[\"auc\"]),\n",
    "    (\"RF\", rf_model, rf_metrics[\"auc\"]),\n",
    "    (\"LR_CV\", cv_model.bestModel, cv_metrics[\"auc\"])\n",
    "]\n",
    "\n",
    "best_name, best_pipe, best_auc = sorted(cand_models, key=lambda x: x[2], reverse=True)[0]\n",
    "print(f\"\\nSelected final model: {best_name} (AUC={best_auc:.4f})\")\n",
    "\n",
    "save_path = f\"/mnt/data/models/{best_name}_pipeline\"\n",
    "best_pipe.write().overwrite().save(save_path)\n",
    "print(f\"Saved pipeline to: {save_path}\")\n",
    "\n",
    "loaded = PipelineModel.load(save_path)\n",
    "loaded_auc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\").evaluate(loaded.transform(test))\n",
    "print(f\"Loaded AUC: {loaded_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4945e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: stop Spark when done\n",
    "# spark.stop()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}